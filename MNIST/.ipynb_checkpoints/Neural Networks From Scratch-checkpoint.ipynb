{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing 2-Layer Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    base = \"C:\\\\GitHub\\\\MLGym\\\\MNIST\\\\\"\n",
    "    f = open(base + name, 'rb')\n",
    "    data = f.read()\n",
    "    magic_number = int((data[0:4]).hex(), 16)\n",
    "    examples = int((data[4:8]).hex(), 16)\n",
    "    mat = []\n",
    "    if magic_number == 2051:\n",
    "        # Images\n",
    "        for i in range(examples):\n",
    "            features = []\n",
    "            for j in range(28*28):\n",
    "                pixel = data[i*28*28 + j + 12]\n",
    "                # Scaling\n",
    "                features.append(pixel/255)\n",
    "            mat.append(features)\n",
    "    else:\n",
    "        # Labels\n",
    "        for i in range(examples):\n",
    "            label = data[i+8]\n",
    "            mat.append(label)\n",
    "    f.close()\n",
    "    return mat\n",
    "\n",
    "train_data_images = \"train-images.idx3-ubyte\"\n",
    "train_data_labels = \"train-labels.idx1-ubyte\"\n",
    "test_data_images = \"t10k-images.idx3-ubyte\"\n",
    "test_data_labels = \"t10k-labels.idx1-ubyte\"\n",
    "\n",
    "X_train = np.array(load_data(train_data_images))\n",
    "y_train = np.array(load_data(train_data_labels))\n",
    "X_test = np.array(load_data(test_data_images))\n",
    "y_test = np.array(load_data(test_data_labels))\n",
    "\n",
    "num_features = X_train.shape[1]\n",
    "num_hidden = 100 # Chosen arbitrarily\n",
    "num_classes = 10 #  Known beforehand\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "layers = [num_features, num_hidden, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784 10 60000 10000 [784, 100, 10]\n"
     ]
    }
   ],
   "source": [
    "print(num_features, num_classes, num_train, num_test, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Digits as Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "id = 0\n",
    "plt.imshow(X_train[id].reshape(28, 28), interpolation='nearest')\n",
    "plt.show()\n",
    "# print(\"Predicted: \", PredSingle(W_pred, b_pred, X_train[id]) , \" Actual: \", y_train[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the ReLU activation function\n",
    "def ReLU(values):\n",
    "    relus = values[:]\n",
    "    relus[values < 0] = 0\n",
    "    return relus\n",
    "\n",
    "# Implementation of the Softmax activation function\n",
    "def Softmax(values):\n",
    "    print(\"n\")\n",
    "    exps = np.exp(values - np.mean(values))\n",
    "    print(\"nn\")\n",
    "    return exps/np.sum(exps, axis = 0, keepdims=True) # All the features are present in a column\n",
    "\n",
    "# Implementation of a generic activation function\n",
    "def Activation(values, name):\n",
    "    if name == \"ReLU\":\n",
    "        return ReLU(values)\n",
    "    elif name == \"Softmax\":\n",
    "        return Softmax(values)\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        raise ValueError(\"Activation Function Not Found\")\n",
    "        \n",
    "def GradActivation(values, name):\n",
    "    if name == \"ReLU\":\n",
    "        return (values > 0)\n",
    "    elif name == \"Softmax\":\n",
    "        sigma = Softmax(values)\n",
    "        act = []\n",
    "        for i in range(len(sigma.shape[1])):\n",
    "            act.append(np.diag(sigma[:, i:i+1]) - np.matmul(sigma[:, i:i+1], sigma[:, i:i+1].T))\n",
    "        return act\n",
    "\n",
    "# Converts a matrix y containing test labels to a hot encoding\n",
    "def HotEncoding(y, num):\n",
    "    code = np.zeros((num, y.shape[0]))\n",
    "    for i in range(y.shape[0]):\n",
    "        code[y[i]][i] = 1\n",
    "    return code\n",
    "\n",
    "# \n",
    "def InitialiseNN(layers):\n",
    "    # Seeding the value for reproducible results\n",
    "    np.random.seed(29)\n",
    "    net = {}\n",
    "    for i in range(len(layers)-1):\n",
    "        # Contains transpose of weight vectors each row size corresponding to next layer and column size to previous layer\n",
    "        weights = np.zeros((layers[i+1], layers[i]))\n",
    "        for j in range(layers[i+1]):\n",
    "            weights[j, :] = np.random.randint(10, size=layers[i])/200\n",
    "        b = np.zeros((layers[i+1], 1))\n",
    "        net[\"W\" + str(i+1)] = weights\n",
    "        net[\"b\" + str(i+1)] = b\n",
    "    return net\n",
    "\n",
    "def Predict(preds):\n",
    "    # Returns the predicted class from the softmax probabilities\n",
    "    prediction = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        prediction.append(np.argmax(preds[:, i]))\n",
    "    return prediction\n",
    "\n",
    "def CrossEntropyLoss(EncodedY, preds):\n",
    "    # Calculates the cross entropy loss by adding up negative log likelihood of softmax probabilities\n",
    "    return -np.squeeze(np.sum(EncodedY * preds))/EncodedY.shape[1]\n",
    "#     loss = 0\n",
    "#     for i in range(len(y_)):\n",
    "#         code = HotEncoding(y_[i:i+1], num_classes)\n",
    "#         loss += np.dot(code.T, np.log(preds[:, i:i+1]))\n",
    "#         loss += np.dot((1-code).T, np.log(1-preds[:, i:i+1]))\n",
    "#     return np.squeeze(-loss/len(y_))\n",
    "\n",
    "def RoundForward(W, b, X, name):\n",
    "    # Returns the activation on all samples in X given W and b and name of the activation function\n",
    "    Z = np.matmul(W, X) + b\n",
    "    print(\"X\", X, name)\n",
    "    return (Activation(Z, name)), Z\n",
    "\n",
    "def UpdateTheta(alpha, X, y, preds, num_classes):\n",
    "    # Updates theta denoted by W here by W[k] = W[k] + (alpha/m)*sum over x(i) .(1{y = k} - softmax(k)) in vectorised format\n",
    "    return alpha*(np.matmul((HotEncoding(y, num_classes) - preds), X))/X.shape[0]\n",
    "\n",
    "def Updateb(alpha, X, y, preds, num_classes):\n",
    "    # Updates b here by b = b + (alpha/m)*sum over (1 {y = k} - softmax(k))\n",
    "    return alpha*np.sum(HotEncoding(y, num_classes) - preds, axis=1, keepdims=True)/X.shape[0]\n",
    "\n",
    "def Update(W, b, alpha, X, y, preds, num_classes):\n",
    "    return W + UpdateTheta(alpha, X, y, preds, num_classes), b + Updateb(alpha, X, y, preds, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LinearFeedForward(Net, X, layers):\n",
    "    cache = {}\n",
    "    l = len(layers) - 2\n",
    "    A = X.T # A is number of features x number of samples (784 , 60000) here\n",
    "    cache[\"A0\"] = X.T\n",
    "    for i in range(0, l):\n",
    "        print(\"LFF X\", X)\n",
    "        A_, Z = RoundForward(Net[\"W\" + str(i+1)], Net[\"b\" + str(i+1)] , A, 'ReLU')\n",
    "        cache[\"A\" + str(i+1)] = A_\n",
    "        cache[\"Z\" + str(i+1)] = Z\n",
    "        A = A_\n",
    "    preds, Z = RoundForward(Net[\"W\" + str(l+1)], Net[\"b\" + str(l+1)] , A, 'Softmax')\n",
    "    cache[\"A\" + str(l+1)] = preds\n",
    "    cache[\"Z\" + str(l+1)] = Z\n",
    "    return preds, cache\n",
    "        \n",
    "def BackPropagation(Net, alpha, EncodedY, cache, preds):\n",
    "    l = len(layers) - 2\n",
    "    AL = cache[\"A\" + str(l+1)]\n",
    "    ZL = cache[\"Z\" + str(l+1)]\n",
    "#     dAL = - np.divide(Y, AL)\n",
    "#     dZ = dAL*GradActivation(ZL, 'Softmax')\n",
    "    dZ = preds[:]\n",
    "    for i in range(EncodedY.shape[0]):\n",
    "        for j in range(EncodedY.shape[1]):\n",
    "            dZ[i][j] -= 1\n",
    "    # Output Layer\n",
    "    dA = np.matmul(Net[\"W\" + str(l+1)].T, dZ)\n",
    "    Net[\"W\" + str(l+1)] = Net[\"W\" + str(l+1)] + alpha * np.matmul(dZ, cache[\"A\" + str(l)].T) / dZ.shape[1]\n",
    "    Net[\"b\" + str(l+1)] = Net[\"b\" + str(l+1)] + alpha * np.squeeze(np.sum(dZ)) / dZ.shape[1]\n",
    "    dA = np.matmul(Net[\"W\" + str(l+1)].T, dZ)\n",
    "    # Hidden Layers\n",
    "    for i in range(l):\n",
    "        Zi = cache[\"Z\" + str(l - i)]\n",
    "        dZ = dA * GradActivation(Zi, 'ReLU')\n",
    "        dA = np.matmul(Net[\"W\" + str(i+1)].T, dZ)\n",
    "        Net[\"W\" + str(i+1)] = Net[\"W\" + str(i+1)] + alpha * np.matmul(dZ, cache[\"A\" + str(l-i - 1)].T) / dZ.shape[1]\n",
    "        Net[\"b\" + str(i+1)] = Net[\"b\" + str(i+1)] + alpha * np.squeeze(np.sum(dZ)) / dZ.shape[1]\n",
    "    return Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2 Layers Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is the learning rate, num_iters is the maximum number of iterations, tol is the tolerance\n",
    "def NeuralNet(alpha, num_iters, tol, X, y, layers):\n",
    "    Net = InitialiseNN(layers)\n",
    "    EncodedY = HotEncoding(y, num_classes)\n",
    "    for i in range(num_iters):\n",
    "        print(\"NN X\", X)\n",
    "        preds, cache = LinearFeedForward(Net, X, layers)\n",
    "        Net = BackPropagation(Net, alpha, EncodedY, cache, preds)\n",
    "        cost = CrossEntropyLoss(preds, EncodedY)\n",
    "        if abs(cost) < tol:\n",
    "            break\n",
    "        if i % 25 == 0:\n",
    "            print(\"Cross Entropy Loss: \", i,  cost)\n",
    "    return Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X [[2.40903922 2.8277451  1.65570588 ... 2.02260784 1.81862745 1.897     ]\n",
      " [2.61831373 2.9667451  1.90409804 ... 2.05445098 1.98554902 2.07019608]\n",
      " [2.47513725 2.63784314 1.68980392 ... 1.69192157 1.69831373 1.74894118]\n",
      " ...\n",
      " [2.37517647 2.75690196 1.76462745 ... 1.84017647 1.83280392 1.64417647]\n",
      " [2.41831373 2.65629412 1.68339216 ... 2.03745098 1.57476471 1.85129412]\n",
      " [2.49256863 2.67776471 1.80313725 ... 1.87252941 1.70521569 1.79882353]]\n",
      "n\n",
      "nn\n",
      "Cross Entropy Loss:  0 0.9003930565462726\n",
      "A [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X [[2615.33682565 2645.05445212 2223.58375217 ... 2433.05651159\n",
      "  2367.50926327 2393.53516537]\n",
      " [2652.99052677 2684.09819415 2241.74046241 ... 2461.44601561\n",
      "  2392.76687903 2420.09648815]\n",
      " [2554.54027826 2581.62279369 2194.50797903 ... 2386.6344842\n",
      "  2326.60404979 2350.49500689]\n",
      " ...\n",
      " [2575.76855528 2603.90289311 2204.78344641 ... 2402.93471213\n",
      "  2341.03055032 2365.42081927]\n",
      " [2543.8231805  2570.56446856 2189.40294421 ... 2378.90730925\n",
      "  2319.33703198 2343.08482932]\n",
      " [2575.04042302 2602.93985325 2204.41574335 ... 2402.32815128\n",
      "  2340.33461354 2364.97929263]]\n",
      "n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn\n",
      "A [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in less\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "n\n",
      "nn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "n\n",
      "nn\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-366268a20424>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Experimentally found that __ is the best learning rate before it fails to converge at alpha = __ onwards.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mNet_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-55ebd26d82db>\u001b[0m in \u001b[0;36mNeuralNet\u001b[1;34m(alpha, num_iters, tol, X, y, layers)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearFeedForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mNet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBackPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEncodedY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEncodedY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-31dcdcc98156>\u001b[0m in \u001b[0;36mBackPropagation\u001b[1;34m(Net, alpha, EncodedY, cache, preds)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mZi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Z\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdA\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mGradActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ReLU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mNet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"A\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mNet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experimentally found that __ is the best learning rate before it fails to converge at alpha = __ onwards. \n",
    "Net_pred = NeuralNet(1, 500, 0.1, X_train, y_train, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.924\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \" ,  np.mean(y_train == Predict(LinearFeedForward(Net_pred, X_train)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9221\n"
     ]
    }
   ],
   "source": [
    "# print(W_pred.shape, b_pred.shape)\n",
    "print(\"Test Accuracy: \" , np.mean(y_test == Predict(LinearFeedForward(Net_pred, X_test)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  1  Actual:  1\n"
     ]
    }
   ],
   "source": [
    "# def PredSingle(W_pred, b_pred, sample):\n",
    "#     return np.argmax(softmax(np.matmul(W_pred, sample.reshape(sample.shape[0], 1)) + b_pred))\n",
    "wrong_indices = [i for i, x in enumerate(y_test != Predict(PredictProbs(W_pred, b_pred, X_test)) ) if x == 1]\n",
    "id = 39\n",
    "plt.imshow(X_test[id].reshape(28, 28), interpolation='nearest')\n",
    "plt.show()\n",
    "print(\"Predicted: \", Predict(PredictProbs(W_pred, b_pred, X_test[id:id+1]))[0] , \" Actual: \", y_test[id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out a resized image from a handwritten digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  1  Actual:  3\n"
     ]
    }
   ],
   "source": [
    "img = np.array(img).reshape(784, 1)\n",
    "plt.imshow(img.reshape(28, 28), interpolation='nearest')\n",
    "plt.show()\n",
    "print(\"Predicted: \", PredSingle(W_pred, b_pred, img) , \" Actual: \", \"3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row index denotes the actual class and Column index denotes predicted class\n",
      "\n",
      "[[ 959    0    3    2    0    4    9    1    2    0]\n",
      " [   0 1109    2    2    0    2    4    2   14    0]\n",
      " [   7    7  920   15   10    3   14   11   38    7]\n",
      " [   3    1   21  918    0   27    2    9   19   10]\n",
      " [   1    2    4    1  917    0   11    2    8   36]\n",
      " [   9    3    3   33    9  772   18    7   31    7]\n",
      " [  11    3    5    1    9   13  911    3    2    0]\n",
      " [   2    9   21    9    6    1    0  944    2   34]\n",
      " [   7    6    7   23    9   24   11   12  869    6]\n",
      " [  10    7    2   11   35    7    0   27    8  902]]\n"
     ]
    }
   ],
   "source": [
    "def ConfusionMatrix(y_test, y_pred):\n",
    "    mat = np.zeros((num_classes, num_classes)).astype(int) # Is a square matrix\n",
    "    for i in range(len(y_test)):\n",
    "        mat[y_test[i]][y_pred[i]] += 1\n",
    "    return mat\n",
    "print(\"Row index denotes the actual class and Column index denotes predicted class\\n\")\n",
    "print(ConfusionMatrix(y_test, Predict(PredictProbs(W_pred, b_pred, X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
