{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layered Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gzip -dk *.gz ## To unzip files, if you are using Linux\n",
    "def load_data(name):\n",
    "    base = \"C:\\\\GitHub\\\\MLGym\\\\MNIST\\\\\"\n",
    "    f = open(base + name, 'rb')\n",
    "    data = f.read()\n",
    "    magic_number = int((data[0:4]).hex(), 16)\n",
    "    examples = int((data[4:8]).hex(), 16)\n",
    "    mat = []\n",
    "    if magic_number == 2051:\n",
    "        # Images\n",
    "        for i in range(examples):\n",
    "            features = []\n",
    "            for j in range(28*28):\n",
    "                pixel = data[i*28*28 + j + 12]\n",
    "                # Scaling\n",
    "                features.append(pixel/255)\n",
    "            mat.append(features)\n",
    "    else:\n",
    "        # Labels\n",
    "        for i in range(examples):\n",
    "            label = data[i+8]\n",
    "            mat.append(label)\n",
    "    f.close()\n",
    "    return mat\n",
    "\n",
    "train_data_images = \"train-images.idx3-ubyte\"\n",
    "train_data_labels = \"train-labels.idx1-ubyte\"\n",
    "test_data_images = \"t10k-images.idx3-ubyte\"\n",
    "test_data_labels = \"t10k-labels.idx1-ubyte\"\n",
    "\n",
    "X_train = np.array(load_data(train_data_images))\n",
    "y_train = np.array(load_data(train_data_labels))\n",
    "X_test = np.array(load_data(test_data_images))\n",
    "y_test = np.array(load_data(test_data_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_hidden = 256 # Chosen arbitrarily\n",
    "num_hidden_2 = 64\n",
    "num_classes = 10 #  Known beforehand\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "layers = [num_features, num_hidden, num_classes] ## For 2 layered neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle calculations of type Y/P; P shouldn't have zero terms\n",
    "def ZeroHandle(ar):\n",
    "    ar[ar < 1e-5 ] = 1e-5\n",
    "    return ar\n",
    "\n",
    "# Convert the labels of images to categorical labels\n",
    "def HotEncoding(Y, num):\n",
    "    # Y = (60000,)  , num = 10\n",
    "    # returns 10 * 60000\n",
    "    EncodedY = np.zeros((num, len(Y)))\n",
    "    for i in range(len(Y)):\n",
    "        EncodedY[Y[i], i] = 1\n",
    "    return EncodedY\n",
    "\n",
    "# ReLU (Rectified Linear Unit) Activation Function defined as ReLU(x) = max(0, x)\n",
    "def ReLU(values):\n",
    "    relus = np.array(values[:])\n",
    "    relus[relus < 0] = 0\n",
    "    return relus\n",
    "\n",
    "# Softmax Activation Function which normalises values after raising them to exponential power \n",
    "def Softmax(values):\n",
    "    maxm = np.max(values, axis=0, keepdims=True)\n",
    "    exps = np.exp(values - maxm)\n",
    "    return ZeroHandle(exps/np.sum(exps, axis=0, keepdims=True))\n",
    "   \n",
    "# Generic activation function to activate based on name   \n",
    "def Activation(values, name):\n",
    "    if name == \"ReLU\":\n",
    "        return ReLU(values)\n",
    "    elif name == \"Softmax\":\n",
    "        return Softmax(values)\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    \n",
    "# Cross Entropy Loss as defined for multiclass softmax regression in the output layer = - Y/P for each sample\n",
    "def CrossEntropyLoss(EncodedY, preds):\n",
    "    return -np.squeeze(np.sum(EncodedY * np.log(preds)))/EncodedY.shape[1]\n",
    "\n",
    "# Returns the gradient of the activation function as a function of the values.\n",
    "def GradActiv(values, name):\n",
    "    if name == \"ReLU\":\n",
    "        grad = values[:]\n",
    "        return (grad >= 0).astype(int)\n",
    "    elif name == \"Softmax\":\n",
    "        sigma = Softmax(values)\n",
    "        act = np.zeros(sigma.shape)\n",
    "        for i in range(sigma.shape[1]):\n",
    "            ar = np.diag(sigma[:, i:i+1]) - np.matmul(sigma[:, i:i+1], sigma[:, i:i+1].T)\n",
    "            act[:, i:i+1] = (np.sum(ar, axis = 1, keepdims=True))\n",
    "        return act\n",
    "        \n",
    "# To return the label having the maximum value in the softmax output layer (interpreted as probability)        \n",
    "def Predict(cache, layers):\n",
    "    h = len(layers) - 2 # Number of hidden layers\n",
    "    preds = cache[\"A\" + str(h+1)]\n",
    "    prediction = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        prediction.append(np.argmax(preds[:, i]))\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To verify above functions\n",
    "# print(HotEncoding([2, 3], 10))\n",
    "print(ReLU([[2, -3, 4], [3, 0.4, -5]]))\n",
    "print(Softmax([[2, -3, 4], [3, 0.4, -5]]))\n",
    "print(Activation([[2, -3, 4], [3, 0.4, -5]], 'ReLU'))\n",
    "EcY = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])\n",
    "P = np.array([[0.2, 0.3, 0.5], [0.1, 0.3, 0.2], [0.7, 0.4, 0.3]])\n",
    "print(\"fv\", np.sum(EcY*np.log(P)))\n",
    "print(CrossEntropyLoss(EcY, P))\n",
    "print(GradActiv(P, 'ReLU'))\n",
    "print(GradActiv(P, 'Softmax'), \"\\n\", EcY)\n",
    "print((GradActiv(P, 'Softmax') * EcY))\n",
    "cac = LinearFeedForward(n, X_train, layers)\n",
    "cdAl = -np.divide(HotEncoding(y_train, num_classes), cac[\"A2\"])\n",
    "print(np.array(GradActiv(cac[\"Z2\"], 'Softmax') * cdAl).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions For Each Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which takes activations from one layer and computes activation of next layer\n",
    "def OneRoundForward(W, X, b, name):\n",
    "    # W = 100 * 784, X = 784 * 60000 (Actual X.T), b = 100 * 1 for 1st layer\n",
    "    Z = np.matmul(W, X) + b\n",
    "    assert(W.shape[0] == b.shape[0])\n",
    "    assert(Z.shape[0] == W.shape[0])\n",
    "    assert(Z.shape[1] == X.shape[1])\n",
    "    # returns 100 * 60000\n",
    "    return Activation(Z, name), Z\n",
    "\n",
    "# Function which takes gradient of inputs of outer/next layer and returns gradient of inputs of inner/prev layer\n",
    "def OneRoundBackward(dA_l, A_prev, W_l, Z_l , name):\n",
    "    # To return dA_prev, dW, db and also calculate dZ\n",
    "    if name == \"Softmax\":\n",
    "        dZ = dA_l\n",
    "    else:\n",
    "        dZ = np.array(dA_l*GradActiv(Z_l, name))\n",
    "    m = dZ.shape[1]\n",
    "    assert(m > 0)\n",
    "    dW = np.matmul(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims=True)/m\n",
    "    dA_prev = np.matmul(W_l.T, dZ)\n",
    "    assert(dZ.shape == Z_l.shape)\n",
    "    assert(dW.shape == W_l.shape)\n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = InitialiseNN(layers)\n",
    "cact, cz = OneRoundForward(n[\"W1\"], X_train.T, n[\"b1\"], 'Softmax')\n",
    "print(np.sum(cact[:, 2:4], axis = 0))\n",
    "# print(cz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that will make up the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns initial values of weights and the biases.\n",
    "# The weights are initialised with small values both positive and negative\n",
    "def InitialiseNN(layers):\n",
    "    # Seeding the value for reproducible results\n",
    "    np.random.seed(60)\n",
    "    net = {}\n",
    "    for i in range(len(layers)-1):\n",
    "        # Contains transpose of weight vectors each row size corresponding to next layer and column size to previous layer\n",
    "        weights = np.zeros((layers[i+1], layers[i]))\n",
    "        for j in range(layers[i+1]):\n",
    "            weights[j, :] = (np.random.randint(10, size=layers[i])-5)/1000\n",
    "        b = np.zeros((layers[i+1], 1))\n",
    "        net[\"W\" + str(i+1)] = weights\n",
    "        net[\"b\" + str(i+1)] = b\n",
    "    return net\n",
    "\n",
    "# Computes one full forward pass and returns a cache storing all intermediate computed values as required\n",
    "def ForwardPass(Net, X, layers):\n",
    "    cache = {}\n",
    "    l = len(layers) - 2 # Number of hidden layers\n",
    "    A0 = X.T\n",
    "    cache[\"A0\"] = A0\n",
    "    for i in range(l):\n",
    "        cache[\"A\" + str(i+1)], cache[\"Z\" + str(i+1)] = OneRoundForward(Net[\"W\" + str(i+1)], cache[\"A\" + str(i)], Net[\"b\" + str(i+1)], 'ReLU')\n",
    "    cache[\"A\" + str(l+1)], cache[\"Z\" + str(l+1)] = OneRoundForward(Net[\"W\" + str(l+1)], cache[\"A\" + str(l)], Net[\"b\" + str(l+1)], 'Softmax')\n",
    "    return cache\n",
    "\n",
    "# Completely back propagates to calculate all the gradients and returns a dictionary storing the gradients\n",
    "def BackwardPass(EncodedY, Net, cache, layers):\n",
    "    h = len(layers) - 2 # l is the number of hidden layers\n",
    "    grads = {}\n",
    "    if 0 in cache[\"A\" + str(h+1)]:\n",
    "        print(\"Error\")\n",
    "    # This value is hardcoded in case of softmax as there was some issue with using dAl = -np.divide(EncodedY, cache[\"A\" + str(h+1)]) and grad(Zl') as defined\n",
    "    dAl = -(EncodedY - cache[\"A\" + str(h+1)])\n",
    "#     dAl = 2*(EncodedY - cache[\"A\" + str(h+1)])/EncodedY.shape[0]\n",
    "#     dAl = -np.divide(EncodedY, cache[\"A\" + str(h+1)])\n",
    "    grads[\"dW\" + str(h+1)], grads[\"db\" + str(h+1)], dA = OneRoundBackward(dAl, cache[\"A\" + str(h)], Net[\"W\" + str(h+1)], cache[\"Z\" + str(h+1)] , 'Softmax') \n",
    "    for i in range(h):\n",
    "        grads[\"dW\" + str(h-i)], grads[\"db\" + str(h-i)], dA_ = OneRoundBackward(dA, cache[\"A\" + str(h-i-1)], Net[\"W\" + str(h-i)], cache[\"Z\" + str(h-i)],'ReLU')\n",
    "        dA = dA_\n",
    "    return grads\n",
    "\n",
    "# Updates all the parameters as per the gradients\n",
    "def UpdateParameters(alpha, grads, Net, layers):\n",
    "    h = len(layers) - 2 # l is the number of hidden layers\n",
    "    for i in range(h+1):\n",
    "        Net[\"W\" + str(i+1)] -= alpha *grads[\"dW\" + str(i+1)]\n",
    "        Net[\"b\" + str(i+1)] -= alpha * grads[\"db\" + str(i+1)]\n",
    "    return Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(InitialiseNN([2, 4, 3]))\n",
    "n = InitialiseNN(layers)\n",
    "print(np.sum(LinearFeedForward(n, X_train, layers)[\"A2\"], axis = 0))\n",
    "print(CrossEntropyLoss(HotEncoding(y_train, num_classes), LinearFeedForward(n, X_train, layers)[\"A2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is the learning rate, num_iters is the maximum number of iterations, tol is the tolerance, CVratio is the ratio of the cross validation set\n",
    "def NeuralNet(alpha, num_iters, tol, X, y, layers, CVratio):\n",
    "    num_samples = X.shape[0]\n",
    "    num_cv = int(CVratio*num_samples)\n",
    "    CrossValidationSet_X = X[:num_cv, :]\n",
    "    CrossValidationSet_y = y[:num_cv]\n",
    "    TrainingSet_X = X[num_cv:, :]\n",
    "    TrainingSet_y = y[num_cv:]\n",
    "    Net = InitialiseNN(layers)\n",
    "    EncodedYTrain = HotEncoding(TrainingSet_y, num_classes)\n",
    "    EncodedYCV = HotEncoding(CrossValidationSet_y, num_classes)\n",
    "    prev_cost = 100\n",
    "    cache = None\n",
    "    for i in range(num_iters):\n",
    "        cache = ForwardPass(Net, TrainingSet_X, layers)\n",
    "        grads = BackwardPass(EncodedYTrain, Net, cache, layers)\n",
    "        Net = UpdateParameters(alpha , grads, Net, layers)\n",
    "        cost = CrossEntropyLoss(EncodedYTrain, cache[\"A\"+str(len(layers)-1)])\n",
    "        CV_Accuracy = np.mean(CrossValidationSet_y == Predict(LinearFeedForward(Net, CrossValidationSet_X, layers), layers))\n",
    "        if cost > prev_cost:\n",
    "            print(cache)\n",
    "            print(i)\n",
    "            return Net, cache\n",
    "        prev_cost = cost\n",
    "        if abs(cost) < tol:\n",
    "            break\n",
    "        if i % 10 == 0:\n",
    "            print(\"Cross Entropy Loss: \", i,  cost, \" Cross Validation Accuracy: \", CV_Accuracy)\n",
    "    \n",
    "    return Net, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Net_pred, cache = NeuralNet(0.2, 1000, 0.1, X_train, y_train, layers, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy: \" ,  np.mean(y_train == Predict(LinearFeedForward(Net_pred, X_train, layers), layers) ))\n",
    "print(\"Test Accuracy: \" , np.mean(y_test == Predict(LinearFeedForward(Net_pred, X_test, layers), layers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(Net_pred, \"parameters.sav\")\n",
    "print(Net_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Digits as Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0\n",
    "plt.imshow(X_train[id].reshape(28, 28), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_test, y_pred):\n",
    "    mat = np.zeros((num_classes, num_classes)).astype(int) # Is a square matrix\n",
    "    for i in range(len(y_test)):\n",
    "        mat[y_test[i]][y_pred[i]] += 1\n",
    "    return mat\n",
    "print(\"Row index denotes the actual class and Column index denotes predicted class\\n\")\n",
    "print(ConfusionMatrix(y_train, Predict(LinearFeedForward(Net_pred, X_train, layers), layers)) )\n",
    "print(ConfusionMatrix(y_test, Predict(LinearFeedForward(Net_pred, X_test, layers), layers)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
